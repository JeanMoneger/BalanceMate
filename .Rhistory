transactions <- as(filtered_df[, -70], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8, maxlen = 4))
# Inspect top rules
inspect(head(sort(rules, by = "lift"), 23))
# Filter out columns with fewer than 2 unique values
filtered_df <- df[, colSums(df == 1) >= 2]
filtered_df[] <- lapply(filtered_df, as.logical)  # Convert to logical (binary)
transactions <- as(filtered_df[, -71], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8, maxlen = 4))
# Inspect top rules
inspect(head(sort(rules, by = "lift"), 23))
transactions <- as(filtered_df[, -71], "transactions")  # Exclude RecipeID and Cluster
transactions <- as(filtered_df[, -71], "transactions")  # Exclude RecipeID and Cluster
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8, maxlen = 4))
# Inspect top rules
inspect(head(sort(rules, by = "lift"), 23))
# Filter out columns with fewer than 2 unique values
filtered_df <- df[, colSums(df == 1) >= 2]
filtered_df[] <- lapply(filtered_df, as.logical)  # Convert to logical (binary)
transactions <- as(filtered_df[, -71], "transactions")  # Exclude RecipeID and Cluster
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8, maxlen = 4))
# Inspect top rules
inspect(head(sort(rules, by = "lift"), 23))
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8, maxlen = 6))
# Inspect top rules
inspect(head(sort(rules, by = "lift"), 23))
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8))
# Inspect top rules
inspect(head(sort(rules, by = "lift"), 23))
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5))
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.8))
# Inspect top rules
inspect(head(sort(rules, by = "lift")))
# Inspect top rules
inspect((sort(rules, by = "lift")))
rules
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
# Inspect top rules
inspect((sort(rules, by = "lift")))
View(df)
# Inspect top rules
inspect((sort(rules, by = "support")))
### GARLIC CLUSTER
Garlic_Cluster <- df[, colSums(df == 1) >= 2 & df$Cluster == 2]
### GARLIC CLUSTER
Garlic_Cluster <- df[df$Cluster == 2, colSums(df == 1) >= 2]
View(Garlic_Cluster)
View(df)
### GARLIC CLUSTER
Garlic_Cluster <- df[df$Cluster == 2, colSums(df == 1) >= 2]
Garlic_Cluster[] <- lapply(Garlic_Cluster, as.logical)  # Convert to logical (binary)
transactions <- as(Garlic_Cluster[, -], "transactions")  # Exclude RecipeID and Cluster
transactions <- as(Garlic_Cluster[, -71], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.2, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
### MEDITTERANEAN CLUSTER
# Filter out columns with fewer than 2 unique values
Med_Cluster <- df[df$Cluster == 3, colSums(df == 1) >= 2]
Med_Cluster[] <- lapply(Med_Cluster, as.logical)  # Convert to logical (binary)
transactions <- as(Med_Cluster[, -71], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
### SOY CLUSTER
# Filter out columns with fewer than 2 unique values
Soy_Cluster <- df[df$Cluster == 1, colSums(df == 1) >= 2]
Soy_Cluster[] <- lapply(Soy_Cluster, as.logical)  # Convert to logical (binary)
transactions <- as(Soy_Cluster[, -71], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.2, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
### GARLIC CLUSTER
Garlic_Cluster <- df[df$Cluster == 2, colSums(df == 1) >= 2]
Garlic_Cluster[] <- lapply(Garlic_Cluster, as.logical)  # Convert to logical (binary)
transactions <- as(Garlic_Cluster[, -71], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.2, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
### MEDITTERANEAN CLUSTER
# Filter out columns with fewer than 2 unique values
Med_Cluster <- df[df$Cluster == 3, colSums(df == 1) >= 2]
Med_Cluster[] <- lapply(Med_Cluster, as.logical)  # Convert to logical (binary)
transactions <- as(Med_Cluster[, -71], "transactions")  # Exclude RecipeID and Cluster
# Calculate item frequencies (absolute count)
item_counts <- itemFrequency(transactions, type = "absolute")
# Filter items that appear in at least 3 transactions
transactions <- transactions[, item_counts >= 3]
# Generate association rules
rules <- apriori(transactions, parameter = list(supp = 0.1, conf = 0.5))
# Inspect top rules
inspect((sort(rules, by = "lift")))
devtools::check()
testthat::test_check()
testthat::test()
getwd()
devtools::test()
devtools::test()
test_that("compute_ellipse_area warns if epochs are unbalanced", {
set.seed(123)
df <- data.frame(
ID = rep("A", 105),        # 10.4 seconds total (approx)
Time = seq(0, 10.4, by = 0.1),
CoP_X = rnorm(106),
CoP_Y = rnorm(106)
)
expect_warning(
compute_ellipse_area(df, "CoP_X", "CoP_Y", "ID", time_col = "Time", epoch_length = 2),
"Unbalanced epochs detected"
)
})
df <- data.frame(
ID = rep("A", 106),        # 10.4 seconds total (approx)
Time = seq(0, 10.4, by = 0.1),
CoP_X = rnorm(106),
CoP_Y = rnorm(106)
)
df <- data.frame(
ID = rep("A", 105),        # 10.4 seconds total (approx)
Time = seq(0, 10.4, by = 0.1),
CoP_X = rnorm(105),
CoP_Y = rnorm(105)
)
devtools::test()
devtools::test()
devtools::test()
df <- data.frame(
ID = rep("A", 100),
Time = seq(0, 9.9, by = 0.1),
CoP_X = rnorm(100),
CoP_Y = rnorm(100)
)
# Epoch length = 2 seconds => 5 epochs for a 10-second recording (0 to 9.9)
result <- compute_ellipse_area(df, "CoP_X", "CoP_Y", "ID", time_col = "Time", epoch_length = 2)
View(df)
View(result)
View(df)
devtools::test()
df <- data.frame(
ID = rep("A", 100),
Time = seq(0.1, 10, by = 0.1),
CoP_X = rnorm(100),
CoP_Y = rnorm(100)
)
# Epoch length = 2 seconds => 5 epochs for a 10-second recording (0 to 9.9)
result <- compute_ellipse_area(df, "CoP_X", "CoP_Y", "ID", time_col = "Time", epoch_length = 2)
# Epoch length = 2 seconds => 5 epochs for a 10-second recording (0 to 9.9)
result <- compute_ellipse_area(df, "CoP_X", "CoP_Y", "ID", time_col = "Time", epoch_length = 1)
# Epoch length = 2 seconds => 5 epochs for a 10-second recording (0 to 9.9)
result <- compute_ellipse_area(df, "CoP_X", "CoP_Y", "ID", time_col = "Time", epoch_length = 5)
test_that("compute_postural_indicators computes EllipseArea correctly", {
df <- data.frame(ID = 1:5, CoP_X = rnorm(5), CoP_Y = rnorm(5))
result <- compute_postural_indicators(
data = df,
CoPX_col = "CoP_X",
CoPY_col = "CoP_Y",
ID = "ID",
indicators = c("EllipseArea")
)
expect_true("EllipseArea" %in% names(result))
expect_equal(result$EllipseArea, BalanceMate::compute_ellipse_area(df, CoPX_col = "CoP_X", CoPY_col = "CoP_Y", ID = "ID"))
})
df <- data.frame(ID = 1:5, CoP_X = rnorm(5), CoP_Y = rnorm(5))
result <- compute_postural_indicators(
data = df,
CoPX_col = "CoP_X",
CoPY_col = "CoP_Y",
ID = "ID",
indicators = c("EllipseArea")
)
expect_true("EllipseArea" %in% names(result))
View(result)
df <- data.frame(ID = 1:5, CoP_X = rnorm(5), CoP_Y = rnorm(5))
View(df)
compute_postural_indicators(
data = df,
CoPX_col = "CoP_X",
CoPY_col = "CoP_Y",
ID = "ID",
indicators = c("EllipseArea")
)
df <- data.frame(ID = rep(1:5, each = 100, CoP_X = rnorm(500), CoP_Y = rnorm(500))
result <- compute_postural_indicators(
df <- data.frame(ID = rep(1:5, each = 100), CoP_X = rnorm(500), CoP_Y = rnorm(500))
result <- compute_postural_indicators(
data = df,
CoPX_col = "CoP_X",
CoPY_col = "CoP_Y",
ID = "ID",
indicators = c("EllipseArea")
)
View(result)
devtools::test()
# Simple case with known values
My <- c(1, 2, 3)
Fz <- c(10, 20, 30)
-2/20
-0.1*100
# Simple case with known values
My <- c(1, 2, 3)
Fz <- c(10, 20, 30)
result <- CoPX_ComputeR(My, Fz)
Mx <- c(-1, 2, -3)
Fz <- c(-10, -20, -30)
result <- CoPY_ComputeR(Mx, Fz)
expected <- c((-1/-10)*100, (2/-20)*100, (-3/-30)*100)  # Mx / Fz * 100
expect_equal(result, expected)
df <- data.frame(
ID = rep("A", 100),
Time = seq(0.1, 10, by = 0.1),
CoP_X = rnorm(100),
CoP_Y = rnorm(100)
)
# Epoch length = 2 seconds => 5 epochs for a 10-second recording (0 to 9.9)
result <- compute_ellipse_area(df, "CoP_X", "CoP_Y", "ID", time_col = "Time", epoch_length = 2)
nrow(df)
table(df$ID)
length(table(df$ID))
100/1
100/100
1/2
0.5%%1
100/2
#'
#'# Compute mean CoP-X and mean CoP-Y at the participant (ID level)
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name")
#'
#' # Synthetise CoP-X and CoP-Y for 1s time bins
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name", time_col = "Time", epoch_length = 1)
#'
#' # Synthetise CoP-X and CoP-Y for 2s time bins
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name", time_col = "Time", epoch_length = 2)
#'
Mean_CoP_ComputeR <- function(data, CoPX_col, CoPY_col, ID, time_col = NULL, epoch_length = NULL) {
# Check if columns exist in the data frame
if (!(CoPX_col %in% colnames(data))) stop(paste("Column", CoPX_col, "not found in the data frame"))
if (!(CoPY_col %in% colnames(data))) stop(paste("Column", CoPY_col, "not found in the data frame"))
if (!(ID %in% colnames(data))) stop(paste("Column", ID, "not found in the data frame"))
# Extract necessary columns
participant_id <- data[[ID]]
# Create grouping variable
if (!is.null(epoch_length) && !is.null(time_col)) {
if (!(time_col %in% colnames(data))) stop(paste("Column", time_col, "not found in the data frame"))
time <- data[[time_col]]
# Create epochs
max_epoch <- ceiling(max(time) / epoch_length)
data$epoch <- pmin(floor(time / epoch_length) + 1, max_epoch)
# Create grouping variable for participant and epoch
data$group <- interaction(participant_id, data$epoch, drop = TRUE)
} else {
# Group by participant only if no epoch is provided
data$group <- as.character(participant_id)
}
# Incomplete epoch warning
# Calculate total duration from time_col
# Assuming time_col starts at 0 for each participant, or that you can subtract min(time_col) from max(time_col).
total_duration <- max(data[[time_col]]) - min(data[[time_col]])
# Compute how many epochs fit in the total duration
number_of_epochs <- total_duration / epoch_length
# Check if this is an integer number of epochs
if (abs(number_of_epochs - round(number_of_epochs)) > .Machine$double.eps^0.5) {
warning("Unbalanced epochs detected -- The total duration is not an integer multiple of the epoch length.")
}
# Split data by group
data_list <- split(data, data$group)
# Function to compute mean CoP-X and CoP-Y for each group
compute_mean_cop <- function(subset_data) {
if (nrow(subset_data) < 1) {
return(c(mean_CoPX = NA, mean_CoPY = NA))
} else {
mean_copx <- mean(subset_data[[CoPX_col]], na.rm = TRUE)
mean_copy <- mean(subset_data[[CoPY_col]], na.rm = TRUE)
return(c(mean_CoPX = mean_copx, mean_CoPY = mean_copy))
}
}
# Apply the function to each group
mean_values <- do.call(rbind, lapply(data_list, compute_mean_cop))
# Prepare the result based on whether epoch was used or not
if (!is.null(epoch_length) && !is.null(time_col)) {
# Split the group back into participant and epoch
group_info <- do.call(rbind, strsplit(names(data_list), split = '\\.'))
result <- data.frame(
participant_id = group_info[, 1],
epoch = as.numeric(group_info[, ncol(group_info)]),
mean_CoPX = mean_values[, "mean_CoPX"],
mean_CoPY = mean_values[, "mean_CoPY"],
stringsAsFactors = FALSE
)
} else {
result <- data.frame(
participant_id = names(data_list),
mean_CoPX = mean_values[, "mean_CoPX"],
mean_CoPY = mean_values[, "mean_CoPY"],
stringsAsFactors = FALSE
)
}
# Order the result based on participant_id and epoch (if applicable)
if ("epoch" %in% colnames(result)) {
result <- result[order(result$participant_id, result$epoch), ]
} else {
result <- result[order(result$participant_id), ]
}
return(result)
}
path_to_data <- system.file("extdata", package = "BalanceMate")
Data <- Merge_PosData(path_to_data, SampleRate = 100, SessionDuration = 100)
Compute mean CoP-X and mean CoP-Y at the participant (ID level)
Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name")
colnames(Data)
Mean_CoP_ComputeR(data = Data, CoPX_col = "CoP_X", CoPY_col = "CoP_Y", ID = "file_name")
#'
#'# Compute mean CoP-X and mean CoP-Y at the participant (ID level)
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name")
#'
#' # Synthetise CoP-X and CoP-Y for 1s time bins
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name", time_col = "Time", epoch_length = 1)
#'
#' # Synthetise CoP-X and CoP-Y for 2s time bins
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name", time_col = "Time", epoch_length = 2)
#'
Mean_CoP_ComputeR <- function(data, CoPX_col, CoPY_col, ID, time_col = NULL, epoch_length = NULL) {
# Check if columns exist in the data frame
if (!(CoPX_col %in% colnames(data))) stop(paste("Column", CoPX_col, "not found in the data frame"))
if (!(CoPY_col %in% colnames(data))) stop(paste("Column", CoPY_col, "not found in the data frame"))
if (!(ID %in% colnames(data))) stop(paste("Column", ID, "not found in the data frame"))
# Extract necessary columns
participant_id <- data[[ID]]
# Create grouping variable
if (!is.null(epoch_length) && !is.null(time_col)) {
if (!(time_col %in% colnames(data))) stop(paste("Column", time_col, "not found in the data frame"))
time <- data[[time_col]]
# Create epochs
max_epoch <- ceiling(max(time) / epoch_length)
data$epoch <- pmin(floor(time / epoch_length) + 1, max_epoch)
# Create grouping variable for participant and epoch
data$group <- interaction(participant_id, data$epoch, drop = TRUE)
} else {
# Group by participant only if no epoch is provided
data$group <- as.character(participant_id)
}
# Incomplete epoch warning
if (!is.null(epoch_length) && !is.null(time_col)) {
if (((nrow(data)/length(table(participant_id)))/100/epoch_length) %% 1 != 0) {
warning("Unbalanced epochs detected -- Epoch is not a multiple of protocol duration")
}
}
# Split data by group
data_list <- split(data, data$group)
# Function to compute mean CoP-X and CoP-Y for each group
compute_mean_cop <- function(subset_data) {
if (nrow(subset_data) < 1) {
return(c(mean_CoPX = NA, mean_CoPY = NA))
} else {
mean_copx <- mean(subset_data[[CoPX_col]], na.rm = TRUE)
mean_copy <- mean(subset_data[[CoPY_col]], na.rm = TRUE)
return(c(mean_CoPX = mean_copx, mean_CoPY = mean_copy))
}
}
# Apply the function to each group
mean_values <- do.call(rbind, lapply(data_list, compute_mean_cop))
# Prepare the result based on whether epoch was used or not
if (!is.null(epoch_length) && !is.null(time_col)) {
# Split the group back into participant and epoch
group_info <- do.call(rbind, strsplit(names(data_list), split = '\\.'))
result <- data.frame(
participant_id = group_info[, 1],
epoch = as.numeric(group_info[, ncol(group_info)]),
mean_CoPX = mean_values[, "mean_CoPX"],
mean_CoPY = mean_values[, "mean_CoPY"],
stringsAsFactors = FALSE
)
} else {
result <- data.frame(
participant_id = names(data_list),
mean_CoPX = mean_values[, "mean_CoPX"],
mean_CoPY = mean_values[, "mean_CoPY"],
stringsAsFactors = FALSE
)
}
# Order the result based on participant_id and epoch (if applicable)
if ("epoch" %in% colnames(result)) {
result <- result[order(result$participant_id, result$epoch), ]
} else {
result <- result[order(result$participant_id), ]
}
return(result)
}
#'
#'# Compute mean CoP-X and mean CoP-Y at the participant (ID level)
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name")
#'
#' # Synthetise CoP-X and CoP-Y for 1s time bins
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name", time_col = "Time", epoch_length = 1)
#'
#' # Synthetise CoP-X and CoP-Y for 2s time bins
#' Mean_CoP_ComputeR(Data, "CoP_X", "CoP_Y", "file_name", time_col = "Time", epoch_length = 2)
#'
Mean_CoP_ComputeR <- function(data, CoPX_col, CoPY_col, ID, time_col = NULL, epoch_length = NULL) {
# Check if columns exist in the data frame
if (!(CoPX_col %in% colnames(data))) stop(paste("Column", CoPX_col, "not found in the data frame"))
if (!(CoPY_col %in% colnames(data))) stop(paste("Column", CoPY_col, "not found in the data frame"))
if (!(ID %in% colnames(data))) stop(paste("Column", ID, "not found in the data frame"))
# Extract necessary columns
participant_id <- data[[ID]]
# Create grouping variable
if (!is.null(epoch_length) && !is.null(time_col)) {
if (!(time_col %in% colnames(data))) stop(paste("Column", time_col, "not found in the data frame"))
time <- data[[time_col]]
# Create epochs
max_epoch <- ceiling(max(time) / epoch_length)
data$epoch <- pmin(floor(time / epoch_length) + 1, max_epoch)
# Create grouping variable for participant and epoch
data$group <- interaction(participant_id, data$epoch, drop = TRUE)
} else {
# Group by participant only if no epoch is provided
data$group <- as.character(participant_id)
}
# Incomplete epoch warning
if (!is.null(epoch_length) && !is.null(time_col)) {
if (((nrow(data)/length(table(participant_id)))/100/epoch_length) %% 1 != 0) {
warning("Unbalanced epochs detected -- Epoch is not a multiple of protocol duration")
}
}
# Split data by group
data_list <- split(data, data$group)
# Function to compute mean CoP-X and CoP-Y for each group
compute_mean_cop <- function(subset_data) {
if (nrow(subset_data) < 1) {
return(c(mean_CoPX = NA, mean_CoPY = NA))
} else {
mean_copx <- mean(subset_data[[CoPX_col]], na.rm = TRUE)
mean_copy <- mean(subset_data[[CoPY_col]], na.rm = TRUE)
return(c(mean_CoPX = mean_copx, mean_CoPY = mean_copy))
}
}
# Apply the function to each group
mean_values <- do.call(rbind, lapply(data_list, compute_mean_cop))
# Prepare the result based on whether epoch was used or not
if (!is.null(epoch_length) && !is.null(time_col)) {
# Split the group back into participant and epoch
group_info <- do.call(rbind, strsplit(names(data_list), split = '\\.'))
result <- data.frame(
participant_id = group_info[, 1],
epoch = as.numeric(group_info[, ncol(group_info)]),
mean_CoPX = mean_values[, "mean_CoPX"],
mean_CoPY = mean_values[, "mean_CoPY"],
stringsAsFactors = FALSE
)
} else {
result <- data.frame(
participant_id = names(data_list),
mean_CoPX = mean_values[, "mean_CoPX"],
mean_CoPY = mean_values[, "mean_CoPY"],
stringsAsFactors = FALSE
)
}
# Order the result based on participant_id and epoch (if applicable)
if ("epoch" %in% colnames(result)) {
result <- result[order(result$participant_id, result$epoch), ]
} else {
result <- result[order(result$participant_id), ]
}
return(result)
}
Mean_CoP_ComputeR(data = Data, CoPX_col = "CoP_X", CoPY_col = "CoP_Y", ID = "file_name")
